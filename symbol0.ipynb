{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f7d18b1-268e-42f4-a094-ae1285b4bcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy'] = \"http://192.41.170.23:3128\" \n",
    "os.environ['https_proxy'] = \"http://192.41.170.23:3128\" \n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c691c39-8fe5-4e4e-93b5-192266f85ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA RTX A6000'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.cuda.get_device_name(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5905b7cc-5f4b-4e6c-9ec7-d67a6a7b81dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "text = \"This is a great [MASK].\"\n",
    "#text = \"The capital of France is [MASK].\"\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53d1437b-c730-4c84-ac50-d41d3c08b0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> This is a great deal.' 3066\n",
      "'>>> This is a great success.' 3112\n",
      "'>>> This is a great adventure.' 6172\n",
      "'>>> This is a great idea.' 2801\n",
      "'>>> This is a great feat.' 8658\n"
     ]
    }
   ],
   "source": [
    "### ALL token Fill\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "token_logits = model(**inputs).logits\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\",token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c2a893-0f80-42f1-beb0-b24c91a2b940",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9c9fdf8-4a37-4667-a45c-6ff80ca4923f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # template must have 3 things : input, symbol, mask token \n",
    "# s1,s2,s3,s4 = \":\" , \">\" , \"-\" , \",\" \n",
    "# mask = \" [MASK]\"\n",
    "# inputs =  \"xxx\" \n",
    "# t1 = inputs + s1 + mask \n",
    "# t2 = inputs + s2 + mask \n",
    "# t3 = inputs + s3 + mask \n",
    "# t4 = inputs + s4 + mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a43939d2-54db-4234-9771-1e5b045ad71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find verbalizer token \n",
    "# ids_labels = tokenizer.convert_tokens_to_ids(['good','bad'])\n",
    "# print(tokenizer.convert_ids_to_tokens(ids_labels) , ids_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93ddc16d-8125-480e-a023-731240e3ce93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForMaskedLM\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# text = \"This is a great [MASK].\"\n",
    "# #text = \"The capital of France is [MASK].\"\n",
    "# model_checkpoint = \"distilbert-base-uncased\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "# model = AutoModelForMaskedLM.from_pretrained(model_checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e132e229-7803-48df-a344-b843d3a71fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = tokenizer(t1,padding=True ,return_tensors=\"pt\").to(device)\n",
    "# token_logits = model(**prompts).logits\n",
    "\n",
    "# # Find the location of [MASK] and extract its logits\n",
    "# mask_token_index = torch.where(prompts[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "# mask_token_logits = token_logits[0, mask_token_index, ids_labels]\n",
    "\n",
    "# # sort \n",
    "# sort_scores = torch.argsort(mask_token_logits,descending=True)\n",
    "# scores = torch.argmax(mask_token_logits)\n",
    "# print(scores)\n",
    "\n",
    "# for idx in sort_scores:\n",
    "#     token = ids_labels[idx]\n",
    "#     print(f\"'{t1.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\",idx)\n",
    "#           #,token_logits[0, mask_token_index, ids_labels[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58f63a0-7b17-4872-a28a-33414ae5fac8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# sst2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e30b9713-48a5-46c3-90eb-eb35cee9324a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sst2 (/home/arnajakt/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9feb75ddc91f4858a45029317cb20410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['idx', 'sentence', 'label'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['idx', 'sentence', 'label'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['idx', 'sentence', 'label'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "sst = load_dataset(\"sst2\")\n",
    "sst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6f7c2af-e6f1-4d37-9ef5-bc061920a7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['idx', 'sentence', 'label'],\n",
       "    num_rows: 67349\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainsst = sst['train']\n",
    "valsst = sst['validation']\n",
    "testsst = sst['test']\n",
    "trainsst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d954f7c-1935-4304-96fe-e11e747c60fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['idx', 'label', 'prompts'],\n",
       "    num_rows: 67349\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = list(map(lambda sentence : sentence + \": \" + \"[MASK]\" , trainsst[\"sentence\"]))\n",
    "trainsst = trainsst.add_column('prompts',prompts)\n",
    "trainsst = trainsst.remove_columns(['sentence'])\n",
    "trainsst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd1d432b-2fe5-4fb9-9d7a-7743bec2a8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/arnajakt/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5/cache-22c6c97846910931.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['idx', 'label', 'prompts', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 67349\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer , AutoModelForMaskedLM\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"prompts\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = trainsst.map(tokenize_function, batched=True)\n",
    "#tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7f720d2-f5f3-48a3-8fbb-ad978349a67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/arnajakt/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5/cache-591166e83dd646fa.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71d2dbbe-d9d5-4133-99d8-d59ee2c10431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making label for Masking task. 0 : negative , 1 : positive \n",
    "def adjust_label(dataset):\n",
    "    # dataset might NOT covert to tensor format.\n",
    "    label_0 = tokenizer(\"negative\")[\"input_ids\"][1]\n",
    "    label_1 = tokenizer(\"positive\")[\"input_ids\"][1]\n",
    "    \n",
    "    pad_length = len(dataset[\"input_ids\"][0])\n",
    "    num_sample = len(dataset)\n",
    "    mock_labels = [[-100]*pad_length]*num_sample\n",
    "    mock_labels = torch.LongTensor(mock_labels)\n",
    "    # x = x.add_column('la99',mock_labels)\n",
    "    dataset.set_format(\"torch\")\n",
    "    \n",
    "    # repalce value in labels array for tranform to MLM patterns\n",
    "    for i in range(len(dataset)) :\n",
    "        cls_token_index = torch.where(dataset[\"input_ids\"][i] == tokenizer.cls_token_id)\n",
    "        cls_token_index = cls_token_index[0].item()\n",
    "        mock_labels[i][cls_token_index] = tokenizer.cls_token_id\n",
    "\n",
    "        sep_token_index = torch.where(dataset[\"input_ids\"][i] == tokenizer.sep_token_id)\n",
    "        sep_token_index = sep_token_index[0].item()\n",
    "        mock_labels[i][sep_token_index] = tokenizer.sep_token_id\n",
    "\n",
    "        mask_token_index = torch.where(dataset[\"input_ids\"][i] == tokenizer.mask_token_id)\n",
    "        mask_token_index = mask_token_index[0].item()\n",
    "        if dataset['label'][i] == 0 : \n",
    "            mock_labels[i][mask_token_index] = label_0\n",
    "        elif  dataset['label'][i] == 1 :\n",
    "            mock_labels[i][mask_token_index] = label_1\n",
    "        else :\n",
    "            assert False , \"something wrong!!\"\n",
    "        \n",
    "        if i == num_sample/2 :\n",
    "            print(\"Half\",i)    \n",
    "    dataset = dataset.add_column('labels',mock_labels.tolist())\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce5184f5-7021-4c59-a9a4-9252d80b2cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/arnajakt/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5/cache-c74fea95c2efd305.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['idx', 'label', 'prompts', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = adjust_label(tokenized_datasets)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a5e4659-b4fb-43f2-81c3-c3c1f61253fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"prompts\",\"idx\",'label'])\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70497b3a-cae5-4362-aa33-88c2330f4279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/arnajakt/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5/cache-ebd389442589be89.arrow\n"
     ]
    }
   ],
   "source": [
    "small_train_dataset = tokenized_datasets.shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9aea8628-a6bc-43e1-b0b9-0849cebb73a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_data(dataset):\n",
    "    prompts = list(map(lambda sentence : sentence + \": \" + \"[MASK]\" , dataset[\"sentence\"]))\n",
    "    dataset = dataset.add_column('prompts',prompts)\n",
    "    dataset = dataset.remove_columns(['sentence'])\n",
    "    \n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_datasets = adjust_label(tokenized_datasets)\n",
    "    tokenized_datasets = tokenized_datasets.remove_columns([\"prompts\",\"idx\",'label'])\n",
    "    \n",
    "    return tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52622479-6252-4a39-ad34-48e5f8a5633a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/arnajakt/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5/cache-6654bfb7d3714800.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half 436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/arnajakt/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5/cache-2da5f972ca5824e5.arrow\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = token_data(valsst)\n",
    "small_eval_dataset = eval_dataset.shuffle(seed=42).select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adac4e48-a7dd-493b-b5e8-814b2d6e3ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 8 \n",
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=batch_size)\n",
    "eval_dataloader = DataLoader(small_eval_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a0746aa-07fc-43e3-8d78-63d5125ac12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 512]),\n",
       " 'attention_mask': torch.Size([8, 512]),\n",
       " 'labels': torch.Size([8, 512])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just check \n",
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "887e053b-ef4b-4415-84e4-1ef842136d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a7c719a-a3bb-4dca-9725-2767532f2c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fe169ea-0479-4073-93ca-0aea5963d761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c06c0d4d-4d5e-410d-b0db-a570452d96ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5677e99e-12e2-4551-adbb-22da2cbfdf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 10000\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6b80bd5-65ed-4d40-b7ad-7fd00e6d8ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA RTX A6000'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "torch.cuda.get_device_name(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d42b46b5-8955-4f6f-899b-3a5200ffb8a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106746999c7043a3b7e394dbbc3b67b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 0: Perplexity: 1.1276384479770414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 1000: Perplexity: 4.162816656315831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 2000: Perplexity: 2.6051282877222635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 3000: Perplexity: 2.7582169052069245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 4000: Perplexity: 1.8482760544030745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 5000: Perplexity: 3.6756523708268816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m     11\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()} \n\u001b[0;32m---> 12\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     14\u001b[0m     accelerator\u001b[38;5;241m.\u001b[39mbackward(loss)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:649\u001b[0m, in \u001b[0;36mDistilBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;124;03m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;124;03m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    647\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 649\u001b[0m dlbrt_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m dlbrt_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    659\u001b[0m prediction_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_transform(hidden_states)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:567\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    566\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(input_ids)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:345\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    343\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_state,)\n\u001b[0;32m--> 345\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:283\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# Self-Attention\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m sa_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    292\u001b[0m     sa_output, sa_weights \u001b[38;5;241m=\u001b[39m sa_output  \u001b[38;5;66;03m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:211\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    209\u001b[0m q \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(dim_per_head)  \u001b[38;5;66;03m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    210\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(q, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m))  \u001b[38;5;66;03m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m mask \u001b[38;5;241m=\u001b[39m (\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m)\u001b[38;5;241m.\u001b[39mview(mask_reshp)\u001b[38;5;241m.\u001b[39mexpand_as(scores)  \u001b[38;5;66;03m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[1;32m    212\u001b[0m scores \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mmasked_fill(\n\u001b[1;32m    213\u001b[0m     mask, torch\u001b[38;5;241m.\u001b[39mtensor(torch\u001b[38;5;241m.\u001b[39mfinfo(scores\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin)\n\u001b[1;32m    214\u001b[0m )  \u001b[38;5;66;03m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[1;32m    216\u001b[0m weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import math\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()} \n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(eval_dataset)]\n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    #print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
    "\n",
    "    # # Save and upload\n",
    "    # accelerator.wait_for_everyone()\n",
    "    # unwrapped_model = accelerator.unwrap_model(model)\n",
    "    # unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    # if accelerator.is_main_process:\n",
    "    #     tokenizer.save_pretrained(output_dir)\n",
    "    #     repo.push_to_hub(\n",
    "    #         commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "    #     )\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
    "        model.save_pretrained(\"model/model_ver1601_epoch\"+str(epoch), from_pt=True)\n",
    "model.save_pretrained(\"model/model_ver1601\", from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6c9222a-99e5-4657-b178-43f24a1440c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = losses\n",
    "l = l.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc5f053f-9638-4747-9c65-65ed7b4e370d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f875fb90310>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa8a2da-0fe6-4d16-a0f4-07e59fe799df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d509fc1e-f3af-497b-a3a8-9bd70345a026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84293d4d-127d-4e67-9b19-047c95bff5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a08a791-9308-4e3b-bb22-dd71a811e1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e5a98b-725d-402e-aeb4-1abedd291976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e352a9-49b9-43e0-80cf-ed6d2bf38f92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d564fd7-abfe-40a2-8a67-ebc9d88d0ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
