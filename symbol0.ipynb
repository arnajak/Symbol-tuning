{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f7d18b1-268e-42f4-a094-ae1285b4bcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy'] = \"http://192.41.170.23:3128\" \n",
    "os.environ['https_proxy'] = \"http://192.41.170.23:3128\" \n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c691c39-8fe5-4e4e-93b5-192266f85ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA RTX A6000'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.cuda.get_device_name(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5905b7cc-5f4b-4e6c-9ec7-d67a6a7b81dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "text = \"This is a great [MASK].\"\n",
    "#text = \"The capital of France is [MASK].\"\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53d1437b-c730-4c84-ac50-d41d3c08b0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> This is a great deal.' 3066\n",
      "'>>> This is a great success.' 3112\n",
      "'>>> This is a great adventure.' 6172\n",
      "'>>> This is a great idea.' 2801\n",
      "'>>> This is a great feat.' 8658\n"
     ]
    }
   ],
   "source": [
    "### ALL token Fill\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "token_logits = model(**inputs).logits\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\",token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c2a893-0f80-42f1-beb0-b24c91a2b940",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9c9fdf8-4a37-4667-a45c-6ff80ca4923f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template must have 3 things : input, symbol, mask token \n",
    "s1,s2,s3,s4 = \":\" , \">\" , \"-\" , \",\" \n",
    "mask = \" [MASK]\"\n",
    "inputs =  \"xxx\" \n",
    "t1 = inputs + s1 + mask \n",
    "t2 = inputs + s2 + mask \n",
    "t3 = inputs + s3 + mask \n",
    "t4 = inputs + s4 + mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a43939d2-54db-4234-9771-1e5b045ad71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'bad'] [2204, 2919]\n"
     ]
    }
   ],
   "source": [
    "# find verbalizer token \n",
    "ids_labels = tokenizer.convert_tokens_to_ids(['good','bad'])\n",
    "print(tokenizer.convert_ids_to_tokens(ids_labels) , ids_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93ddc16d-8125-480e-a023-731240e3ce93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "text = \"This is a great [MASK].\"\n",
    "#text = \"The capital of France is [MASK].\"\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e132e229-7803-48df-a344-b843d3a71fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0, device='cuda:0')\n",
      "'xxx: good' tensor(0, device='cuda:0')\n",
      "'xxx: bad' tensor(1, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "prompts = tokenizer(t1,padding=True ,return_tensors=\"pt\").to(device)\n",
    "token_logits = model(**prompts).logits\n",
    "\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(prompts[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, ids_labels]\n",
    "\n",
    "# sort \n",
    "sort_scores = torch.argsort(mask_token_logits,descending=True)\n",
    "scores = torch.argmax(mask_token_logits)\n",
    "print(scores)\n",
    "\n",
    "for idx in sort_scores:\n",
    "    token = ids_labels[idx]\n",
    "    print(f\"'{t1.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\",idx)\n",
    "          #,token_logits[0, mask_token_index, ids_labels[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58f63a0-7b17-4872-a28a-33414ae5fac8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# sst2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e30b9713-48a5-46c3-90eb-eb35cee9324a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sst2 (/home/arnajakt/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8277ba90fa490889b7ef475f80275e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['idx', 'sentence', 'label'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['idx', 'sentence', 'label'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['idx', 'sentence', 'label'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "sst = load_dataset(\"sst2\")\n",
    "sst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6f7c2af-e6f1-4d37-9ef5-bc061920a7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['idx', 'sentence', 'label'],\n",
       "    num_rows: 67349\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainsst = sst['train']\n",
    "valsst = sst['validation']\n",
    "testsst = sst['test']\n",
    "trainsst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d954f7c-1935-4304-96fe-e11e747c60fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['idx', 'label', 'prompts'],\n",
       "    num_rows: 67349\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = list(map(lambda sentence : sentence + \": \" + \"[MASK]\" , trainsst[\"sentence\"]))\n",
    "trainsst = trainsst.add_column('prompts',prompts)\n",
    "trainsst = trainsst.remove_columns(['sentence'])\n",
    "trainsst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd1d432b-2fe5-4fb9-9d7a-7743bec2a8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/arnajakt/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5/cache-22c6c97846910931.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['idx', 'label', 'prompts', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 67349\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer , AutoModelForMaskedLM\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"prompts\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = trainsst.map(tokenize_function, batched=True)\n",
    "#tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7f720d2-f5f3-48a3-8fbb-ad978349a67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/arnajakt/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5/cache-591166e83dd646fa.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71d2dbbe-d9d5-4133-99d8-d59ee2c10431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making label for Masking task. 0 : negative , 1 : positive \n",
    "def adjust_label(dataset):\n",
    "    # dataset might NOT covert to tensor format.\n",
    "    label_0 = tokenizer(\"negative\")[\"input_ids\"][1]\n",
    "    label_1 = tokenizer(\"positive\")[\"input_ids\"][1]\n",
    "    \n",
    "    pad_length = len(dataset[\"input_ids\"][0])\n",
    "    num_sample = len(dataset)\n",
    "    mock_labels = [[-100]*pad_length]*num_sample\n",
    "    mock_labels = torch.LongTensor(mock_labels)\n",
    "    # x = x.add_column('la99',mock_labels)\n",
    "    dataset.set_format(\"torch\")\n",
    "    \n",
    "    # repalce value in labels array for tranform to MLM patterns\n",
    "    for i in range(len(dataset)) :\n",
    "        cls_token_index = torch.where(dataset[\"input_ids\"][i] == tokenizer.cls_token_id)\n",
    "        cls_token_index = cls_token_index[0].item()\n",
    "        mock_labels[i][cls_token_index] = tokenizer.cls_token_id\n",
    "\n",
    "        sep_token_index = torch.where(dataset[\"input_ids\"][i] == tokenizer.sep_token_id)\n",
    "        sep_token_index = sep_token_index[0].item()\n",
    "        mock_labels[i][sep_token_index] = tokenizer.sep_token_id\n",
    "\n",
    "        mask_token_index = torch.where(dataset[\"input_ids\"][i] == tokenizer.mask_token_id)\n",
    "        mask_token_index = mask_token_index[0].item()\n",
    "        if dataset['label'][i] == 0 : \n",
    "            mock_labels[i][mask_token_index] = label_0\n",
    "        elif  dataset['label'][i] == 1 :\n",
    "            mock_labels[i][mask_token_index] = label_1\n",
    "        else :\n",
    "            assert False , \"something wrong!!\"\n",
    "        \n",
    "        if i == num_sample/2 :\n",
    "            print(\"Half\",i)    \n",
    "    dataset = dataset.add_column('labels',mock_labels.tolist())\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce5184f5-7021-4c59-a9a4-9252d80b2cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half 500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "814a073de36d4442bc221c18bce0419f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['idx', 'label', 'prompts', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = adjust_label(tokenized_datasets)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a5e4659-b4fb-43f2-81c3-c3c1f61253fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"prompts\",\"idx\",'label'])\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70497b3a-cae5-4362-aa33-88c2330f4279",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets.shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9aea8628-a6bc-43e1-b0b9-0849cebb73a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_data(dataset):\n",
    "    prompts = list(map(lambda sentence : sentence + \": \" + \"[MASK]\" , dataset[\"sentence\"]))\n",
    "    dataset = dataset.add_column('prompts',prompts)\n",
    "    dataset = dataset.remove_columns(['sentence'])\n",
    "    \n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_datasets = adjust_label(tokenized_datasets)\n",
    "    tokenized_datasets = tokenized_datasets.remove_columns([\"prompts\",\"idx\",'label'])\n",
    "    \n",
    "    return tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52622479-6252-4a39-ad34-48e5f8a5633a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/arnajakt/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5/cache-6654bfb7d3714800.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half 436\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = token_data(valsst)\n",
    "small_eval_dataset = eval_dataset.shuffle(seed=42).select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adac4e48-a7dd-493b-b5e8-814b2d6e3ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 8 \n",
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=batch_size)\n",
    "eval_dataloader = DataLoader(small_eval_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a0746aa-07fc-43e3-8d78-63d5125ac12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 512]),\n",
       " 'attention_mask': torch.Size([8, 512]),\n",
       " 'labels': torch.Size([8, 512])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just check \n",
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "887e053b-ef4b-4415-84e4-1ef842136d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a7c719a-a3bb-4dca-9725-2767532f2c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fe169ea-0479-4073-93ca-0aea5963d761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c06c0d4d-4d5e-410d-b0db-a570452d96ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5677e99e-12e2-4551-adbb-22da2cbfdf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6b80bd5-65ed-4d40-b7ad-7fd00e6d8ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA RTX A6000'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "torch.cuda.get_device_name(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d42b46b5-8955-4f6f-899b-3a5200ffb8a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ffcebd088e643c3b002bc1eb8d6da61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 0: Perplexity: 1.1190520430832305\n",
      ">>> Epoch 1: Perplexity: 1.1244929383979811\n",
      ">>> Epoch 2: Perplexity: 1.1244929383979811\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import math\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()} \n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(eval_dataset)]\n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
    "\n",
    "    # # Save and upload\n",
    "    # accelerator.wait_for_everyone()\n",
    "    # unwrapped_model = accelerator.unwrap_model(model)\n",
    "    # unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    # if accelerator.is_main_process:\n",
    "    #     tokenizer.save_pretrained(output_dir)\n",
    "    #     repo.push_to_hub(\n",
    "    #         commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ab567a-843f-4339-98e0-0d77a45df659",
   "metadata": {},
   "outputs": [],
   "source": [
    "x['input_ids'].get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825fa1db-abfa-4f7a-92ad-f938efb68138",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6382f3-66bb-48db-9c05-4e503730493f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034162e9-0a4f-4a85-9104-05dea1e60138",
   "metadata": {},
   "source": [
    "RuntimeError: philox_cuda_state for an unexpected CUDA generator used during capture. In regions captured by CUDA graphs, you may only use the default CUDA RNG generator on the device that's current when capture begins. If you need a non-default (user-supplied) generator, or a generator on another device, please file an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c9222a-99e5-4657-b178-43f24a1440c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b930c4df-2eac-4675-958b-3c0b41c5bfef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5f053f-9638-4747-9c65-65ed7b4e370d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa8a2da-0fe6-4d16-a0f4-07e59fe799df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
